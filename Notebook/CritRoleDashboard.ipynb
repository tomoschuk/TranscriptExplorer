{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critical Role dashboard script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the accompanying script for crtranscriptexplorer.herokuapp.com. In it, I take a file a of text files (subtitles from Critical Role, a popular Dungeons and Dragons vodcast), clean and process the words, and output a graph showing how frequently each word or phrase is used. First we load all of the necessary packages and build an NLP pipeline.\n",
    "\n",
    "Data can be found at crtranscript.tumblr.com. Major credit goes to that group (unaffiliated) for annotating all of the episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "stop_words =  set(stopwords.words('english'))\n",
    "\n",
    "def is_punct_space(token):\n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "direc = '../FinalTextFiles/'\n",
    "newStopWords = ['-PRON-']\n",
    "\n",
    "\n",
    "plotly.tools.set_credentials_file(username='btomoschuk', api_key='EDkuHeNiiXQBG92SPAz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open and read the txt into dataframes\n",
    "os.chdir(direc)\n",
    "text = []\n",
    "ffiles = [f for f in os.listdir(direc) if os.path.isfile(f)]\n",
    "for f in ffiles:\n",
    "    with io.open (f, \"r\", encoding = 'cp437') as myfile:\n",
    "        text.append(myfile.read())\n",
    "\n",
    "data = pd.DataFrame({'raw':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This code is adapted from http://acsweb.ucsd.edu/~btomosch/ironfist.html\n",
    "#This function cleans up the dataframe, removes weird characters and spaces\n",
    "def cleaning_pipeline(data):\n",
    "    data = data.reset_index(drop=True)\n",
    "    #Turns line breaks into spaces and replaces messed up apostrophe\n",
    "    data['raw'] = [r.replace('\\n', ' ') for r in data['raw']]\n",
    "    data['raw'] = [r.replace('ΓÇÖ', \"'\") for r in data['raw']]\n",
    "    data['raw'] = data['raw'].astype(str)\n",
    "    #Split files by searching for Names in all uppercase + :\n",
    "    data = data.iloc[1:]\n",
    "    data['split'] = [re.split(r\"\\s(?=[A-Z]+:)\",x) for x in data['raw']]\n",
    "    #Add episodes\n",
    "    data = pd.DataFrame(data.split.values.tolist(), index= data.index)\n",
    "    data['episode'] = data.index\n",
    "    data = pd.melt(data, id_vars=['episode'])\n",
    "    data = pd.concat([data, data['value'].str.split(':',1, expand = True)], axis=1)\n",
    "    data = data[['episode',0,1]].dropna().rename(index=str, columns={0: \"Speaker\", 1: \"Speech\"})\n",
    "    data['Speech'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "    return data\n",
    "\n",
    "#This parses all of the text and removes custom stop words\n",
    "def nlp_pipeline(data):\n",
    "    #Parse and lemmatize the data\n",
    "    data['parsed'] = [ nlp(x) for x in data.Speech]\n",
    "    data['lemmatized'] = [[token.lemma_ for token in x\n",
    "                            if not is_punct_space(token)] \n",
    "                  for x in data.parsed]\n",
    "    \n",
    "    #Remove Stop words\n",
    "    data['cleaned'] = [[term for term in x\n",
    "                                if not term in newStopWords]\n",
    "                               for x in data.lemmatized]\n",
    "    #Collapse into one string rather than list\n",
    "    data.cleaned = [' '.join(x) for x in data.cleaned]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Speech</th>\n",
       "      <th>parsed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Hello everyone. My name is MATThew Mercer,   ...</td>\n",
       "      <td>( , Hello, everyone, ., My, name, is, MATThew,...</td>\n",
       "      <td>[hello, everyone, -PRON-, name, be, matthew, m...</td>\n",
       "      <td>hello everyone name be matthew mercer voice ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Hello everyone, and welcome to the second epi...</td>\n",
       "      <td>( , Hello, everyone, ,, and, welcome, to, the,...</td>\n",
       "      <td>[hello, everyone, and, welcome, to, the, secon...</td>\n",
       "      <td>hello everyone and welcome to the second episo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Hey, everyone. Sorry about that little issue ...</td>\n",
       "      <td>( , Hey, ,, everyone, ., Sorry, about, that, l...</td>\n",
       "      <td>[hey, everyone, sorry, about, that, little, is...</td>\n",
       "      <td>hey everyone sorry about that little issue the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Everyone, welcome to the new episode of Criti...</td>\n",
       "      <td>( , Everyone, ,, welcome, to, the, new, episod...</td>\n",
       "      <td>[everyone, welcome, to, the, new, episode, of,...</td>\n",
       "      <td>everyone welcome to the new episode of critica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Hello everyone, welcome to the fifth episode ...</td>\n",
       "      <td>( , Hello, everyone, ,, welcome, to, the, fift...</td>\n",
       "      <td>[hello, everyone, welcome, to, the, fifth, epi...</td>\n",
       "      <td>hello everyone welcome to the fifth episode of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Hello everyone! Welcome to Critical Role toni...</td>\n",
       "      <td>( , Hello, everyone, !, Welcome, to, Critical,...</td>\n",
       "      <td>[hello, everyone, welcome, to, critical, role,...</td>\n",
       "      <td>hello everyone welcome to critical role tonigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Hello, and welcome to this evening's episode ...</td>\n",
       "      <td>( , Hello, ,, and, welcome, to, this, evening,...</td>\n",
       "      <td>[hello, and, welcome, to, this, evening, 's, e...</td>\n",
       "      <td>hello and welcome to this evening 's episode o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Hello and good evening, everyone. Welcome to ...</td>\n",
       "      <td>( , Hello, and, good, evening, ,, everyone, .,...</td>\n",
       "      <td>[hello, and, good, evening, everyone, welcome,...</td>\n",
       "      <td>hello and good evening everyone welcome to ton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Welcome and good evening, everyone. Welcome t...</td>\n",
       "      <td>( , Welcome, and, good, evening, ,, everyone, ...</td>\n",
       "      <td>[welcome, and, good, evening, everyone, welcom...</td>\n",
       "      <td>welcome and good evening everyone welcome to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Welcome, everyone, to this  Thursday's episod...</td>\n",
       "      <td>( , Welcome, ,, everyone, ,, to, this,  , Thur...</td>\n",
       "      <td>[welcome, everyone, to, this, thursday, 's, ep...</td>\n",
       "      <td>welcome everyone to this thursday 's episode o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Good evening, everyone! And welcome to our el...</td>\n",
       "      <td>( , Good, evening, ,, everyone, !, And, welcom...</td>\n",
       "      <td>[good, evening, everyone, and, welcome, to, -P...</td>\n",
       "      <td>good evening everyone and welcome to eleventh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Welcome back, everyone, to the next episode o...</td>\n",
       "      <td>( , Welcome, back, ,, everyone, ,, to, the, ne...</td>\n",
       "      <td>[welcome, back, everyone, to, the, next, episo...</td>\n",
       "      <td>welcome back everyone to the next episode of c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Hello everyone and welcome  to this evening's...</td>\n",
       "      <td>( , Hello, everyone, and, welcome,  , to, this...</td>\n",
       "      <td>[hello, everyone, and, welcome, to, this, even...</td>\n",
       "      <td>hello everyone and welcome to this evening 's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Hello everyone and welcome back to Critical R...</td>\n",
       "      <td>( , Hello, everyone, and, welcome, back, to, C...</td>\n",
       "      <td>[hello, everyone, and, welcome, back, to, crit...</td>\n",
       "      <td>hello everyone and welcome back to critical ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Hello everyone and welcome back to Critical R...</td>\n",
       "      <td>( , Hello, everyone, and, welcome, back, to, C...</td>\n",
       "      <td>[hello, everyone, and, welcome, back, to, crit...</td>\n",
       "      <td>hello everyone and welcome back to critical ro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode Speaker                                             Speech  \\\n",
       "0         1    MATT   Hello everyone. My name is MATThew Mercer,   ...   \n",
       "1         2    MATT   Hello everyone, and welcome to the second epi...   \n",
       "2         3    MATT   Hey, everyone. Sorry about that little issue ...   \n",
       "3         4    MATT   Everyone, welcome to the new episode of Criti...   \n",
       "4         5    MATT   Hello everyone, welcome to the fifth episode ...   \n",
       "5         6    MATT   Hello everyone! Welcome to Critical Role toni...   \n",
       "6         7    MATT   Hello, and welcome to this evening's episode ...   \n",
       "7         8    MATT   Hello and good evening, everyone. Welcome to ...   \n",
       "8         9    MATT   Welcome and good evening, everyone. Welcome t...   \n",
       "9        10    MATT   Welcome, everyone, to this  Thursday's episod...   \n",
       "10       11    MATT   Good evening, everyone! And welcome to our el...   \n",
       "12       13    MATT   Welcome back, everyone, to the next episode o...   \n",
       "13       14    MATT   Hello everyone and welcome  to this evening's...   \n",
       "14       15    MATT   Hello everyone and welcome back to Critical R...   \n",
       "15       16    MATT   Hello everyone and welcome back to Critical R...   \n",
       "\n",
       "                                               parsed  \\\n",
       "0   ( , Hello, everyone, ., My, name, is, MATThew,...   \n",
       "1   ( , Hello, everyone, ,, and, welcome, to, the,...   \n",
       "2   ( , Hey, ,, everyone, ., Sorry, about, that, l...   \n",
       "3   ( , Everyone, ,, welcome, to, the, new, episod...   \n",
       "4   ( , Hello, everyone, ,, welcome, to, the, fift...   \n",
       "5   ( , Hello, everyone, !, Welcome, to, Critical,...   \n",
       "6   ( , Hello, ,, and, welcome, to, this, evening,...   \n",
       "7   ( , Hello, and, good, evening, ,, everyone, .,...   \n",
       "8   ( , Welcome, and, good, evening, ,, everyone, ...   \n",
       "9   ( , Welcome, ,, everyone, ,, to, this,  , Thur...   \n",
       "10  ( , Good, evening, ,, everyone, !, And, welcom...   \n",
       "12  ( , Welcome, back, ,, everyone, ,, to, the, ne...   \n",
       "13  ( , Hello, everyone, and, welcome,  , to, this...   \n",
       "14  ( , Hello, everyone, and, welcome, back, to, C...   \n",
       "15  ( , Hello, everyone, and, welcome, back, to, C...   \n",
       "\n",
       "                                           lemmatized  \\\n",
       "0   [hello, everyone, -PRON-, name, be, matthew, m...   \n",
       "1   [hello, everyone, and, welcome, to, the, secon...   \n",
       "2   [hey, everyone, sorry, about, that, little, is...   \n",
       "3   [everyone, welcome, to, the, new, episode, of,...   \n",
       "4   [hello, everyone, welcome, to, the, fifth, epi...   \n",
       "5   [hello, everyone, welcome, to, critical, role,...   \n",
       "6   [hello, and, welcome, to, this, evening, 's, e...   \n",
       "7   [hello, and, good, evening, everyone, welcome,...   \n",
       "8   [welcome, and, good, evening, everyone, welcom...   \n",
       "9   [welcome, everyone, to, this, thursday, 's, ep...   \n",
       "10  [good, evening, everyone, and, welcome, to, -P...   \n",
       "12  [welcome, back, everyone, to, the, next, episo...   \n",
       "13  [hello, everyone, and, welcome, to, this, even...   \n",
       "14  [hello, everyone, and, welcome, back, to, crit...   \n",
       "15  [hello, everyone, and, welcome, back, to, crit...   \n",
       "\n",
       "                                              cleaned  \n",
       "0   hello everyone name be matthew mercer voice ac...  \n",
       "1   hello everyone and welcome to the second episo...  \n",
       "2   hey everyone sorry about that little issue the...  \n",
       "3   everyone welcome to the new episode of critica...  \n",
       "4   hello everyone welcome to the fifth episode of...  \n",
       "5   hello everyone welcome to critical role tonigh...  \n",
       "6   hello and welcome to this evening 's episode o...  \n",
       "7   hello and good evening everyone welcome to ton...  \n",
       "8   welcome and good evening everyone welcome to t...  \n",
       "9   welcome everyone to this thursday 's episode o...  \n",
       "10  good evening everyone and welcome to eleventh ...  \n",
       "12  welcome back everyone to the next episode of c...  \n",
       "13  hello everyone and welcome to this evening 's ...  \n",
       "14  hello everyone and welcome back to critical ro...  \n",
       "15  hello everyone and welcome back to critical ro...  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = cleaning_pipeline(data)\n",
    "data = nlp_pipeline(data)\n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! The nlp_pipeline function takes a long time, since we're building a dataframe with multiple copies of the parsed data in it. Only the episode, speaker, and cleaned columns are ultimately necessary, but I like showing the process from raw text to a single string of language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a dataframe of the counts for each word and speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The words we want to search for in the text\n",
    "words = [string.lower() for string in ['dungeon','dragon','dice','weapon','oh no', 'awesome']]\n",
    "\n",
    "#The people we want to include in the analysis\n",
    "peeps = [string.upper() for string in ['Sam','Laura','Travis','Matt','Liam','Taliesin','Marisha', 'Ashley']]\n",
    "\n",
    "\n",
    "#Use this to pickle the data - the dash code uses this file\n",
    "#data[['episode','Speaker','cleaned']].to_pickle(\"../cleandata.pk1\")\n",
    "\n",
    "#Use this to filter out episodes\n",
    "data_subset = data[data['episode'] >= 1][data['episode'] <= 115]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only need to go to here and pickle the data if you use the Dash script to generate a python app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAM</td>\n",
       "      <td>go log on to geekandsundry.com slash find on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LAURA</td>\n",
       "      <td>everybody except for some people oh have alrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAVIS</td>\n",
       "      <td>right listen up if have ale then have a friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MATT</td>\n",
       "      <td>hello everyone name be matthew mercer voice ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LIAM</td>\n",
       "      <td>between 1:00 and 4:00 be when be do stuff be a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TALIESIN</td>\n",
       "      <td>pissblossom cock goblin hi ugh product no be g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MARISHA</td>\n",
       "      <td>speak of the wills just to talk about the firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ASHLEY</td>\n",
       "      <td>pike grow up in the outskirt of town near the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    speaker                                               text\n",
       "0       SAM  go log on to geekandsundry.com slash find on t...\n",
       "1     LAURA  everybody except for some people oh have alrea...\n",
       "2    TRAVIS  right listen up if have ale then have a friend...\n",
       "3      MATT  hello everyone name be matthew mercer voice ac...\n",
       "4      LIAM  between 1:00 and 4:00 be when be do stuff be a...\n",
       "5  TALIESIN  pissblossom cock goblin hi ugh product no be g...\n",
       "6   MARISHA  speak of the wills just to talk about the firs...\n",
       "7    ASHLEY  pike grow up in the outskirt of town near the ..."
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge all episodes into one string per speaker using a dictionary\n",
    "fullmerge = {}\n",
    "for peep in peeps:\n",
    "    fullmerge.update({peep:' '.join(data_subset[data_subset.Speaker == peep]['cleaned'])})\n",
    "finaldata = pd.DataFrame.from_dict(fullmerge, orient = 'index').reset_index().rename(columns = {'index':'speaker',0:'text'})\n",
    "finaldata.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a dataframe and populate it with the numbers for each of the above words for each person\n",
    "freq = pd.DataFrame()\n",
    "for peep in peeps:\n",
    "    for word in words:\n",
    "        freq = freq.append(pd.Series([peep, word, finaldata[finaldata['speaker'] == peep].text.str.count(word).iloc[0]]), ignore_index = True)\n",
    "        \n",
    "freq = freq.rename(index=str, columns={0: \"speaker\", 1: \"word\",2: \"amount\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>word</th>\n",
       "      <th>amount</th>\n",
       "      <th>total</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ASHLEY</td>\n",
       "      <td>bigby</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54439.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>LIAM</td>\n",
       "      <td>bad news</td>\n",
       "      <td>3.0</td>\n",
       "      <td>228305.0</td>\n",
       "      <td>0.013140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SAM</td>\n",
       "      <td>bad news</td>\n",
       "      <td>4.0</td>\n",
       "      <td>241442.0</td>\n",
       "      <td>0.016567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LAURA</td>\n",
       "      <td>bad news</td>\n",
       "      <td>6.0</td>\n",
       "      <td>290054.0</td>\n",
       "      <td>0.020686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>TALIESIN</td>\n",
       "      <td>bigby</td>\n",
       "      <td>5.0</td>\n",
       "      <td>240624.0</td>\n",
       "      <td>0.020779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     speaker      word  amount     total      rate\n",
       "49    ASHLEY     bigby     0.0   54439.0  0.000000\n",
       "33      LIAM  bad news     3.0  228305.0  0.013140\n",
       "5        SAM  bad news     4.0  241442.0  0.016567\n",
       "12     LAURA  bad news     6.0  290054.0  0.020686\n",
       "35  TALIESIN     bigby     5.0  240624.0  0.020779"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the total number of words spoken by each speaker - estimated as the number of spaces.\n",
    "totalwords = pd.DataFrame()\n",
    "for peep in peeps:\n",
    "    totalwords = totalwords.append(pd.Series([peep, ' ', finaldata[finaldata['speaker'] == peep].text.str.count(' ').iloc[0]]), ignore_index = True)\n",
    "\n",
    "totalwords = totalwords.rename(index=str, columns={0: \"speaker\", 1: \"word\",2: \"total\"})\n",
    "\n",
    "freq = pd.merge(freq, totalwords[['speaker','total']], on='speaker')\n",
    "\n",
    "#Calculate the rate of times the word is said per 1000 words\n",
    "freq['rate'] = (freq['amount']/freq['total'])*1000\n",
    "totalwords = None\n",
    "\n",
    "#sort by rate and speaker\n",
    "freq = freq.sort_values(by=['rate','speaker'], ascending = [True,False])\n",
    "freq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have a dataframe that shows the frequency and rates of each word for each speaker. Lastly we will make the graph with plotly, since the dashboard package (Dash) uses plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the tfidf function, and find the most \"distinguishing\" word among the given words and speakers\n",
    "tfidf = TfidfVectorizer(stop_words='english', vocabulary = words)\n",
    "tfs = tfidf.fit_transform(finaldata['text'])\n",
    "matrix = pd.DataFrame(tfs.todense(), index = peeps, columns = tfidf.get_feature_names()).transpose()\n",
    "matrix['word'] = matrix.index\n",
    "matrix = pd.melt(matrix, id_vars = 'word')\n",
    "matrix = matrix.rename(index=str, columns={'value': \"tfidf\",'variable': \"speaker\"})\n",
    "distWord = matrix.loc[matrix['tfidf'].idxmax()]['word']\n",
    "distSpeaker = matrix.loc[matrix['tfidf'].idxmax()]['speaker']\n",
    "tfidfSent = (\"Most distinguishing: '\" + distWord + \"' by \" + distSpeaker + \".*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~btomoschuk/4.embed\" height=\"600px\" width=\"800px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = ff.create_facet_grid(\n",
    "    freq,\n",
    "    x='rate',\n",
    "    y='word',\n",
    "    facet_col='speaker',\n",
    "    color_name='speaker',\n",
    "    trace_type='bar',\n",
    "    orientation = 'h',\n",
    "    ggplot2 = True\n",
    ")\n",
    "\n",
    "#Change the axes to match the minimum rate and 15% over the maximum rate for nice graphing\n",
    "for i in range(len(peeps)+1):\n",
    "    if i == 0:\n",
    "        fig.layout.xaxis.update({'range': [freq['rate'].min(), (freq['rate'].max()+(.15 * freq['rate'].max()))]})\n",
    "    else:\n",
    "        exec('fig.layout.xaxis' + str(i)+\".update({'range': [freq['rate'].min(), (freq['rate'].max()+(.15 * freq['rate'].max()))]})\")\n",
    "\n",
    "fig.layout.xaxis.title = tfidfSent\n",
    "fig.layout.update(plot_bgcolor='rgba(230,230,230,90)')\n",
    "py.iplot(fig, filename='critrole')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make dashboard handle other scripts\n",
    "    * This will involve streamlining the NLP pipeline as much as possible\n",
    "* Add figure that tracks word use per episode over some season\n",
    "* Offer non-lematized data option for searching n-grams\n",
    "    * Current system allows for better searching of content, but will fail to capture common phrases or sentences\n",
    "* Get feedback from the fan community\n",
    "\n",
    "See github.com/tomoschuk for the accompanying Dash script that generates crtranscript.herokuapp.com!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
