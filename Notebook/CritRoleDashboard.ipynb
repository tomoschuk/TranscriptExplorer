{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critical Role dashboard script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the accompanying script for crtranscriptexplorer.herokuapp.com. In it, I take a file a of text files (subtitles from Critical Role, a popular Dungeons and Dragons vodcast), clean and process the words, and output a graph showing how frequently each word or phrase is used. First we load all of the necessary packages and build an NLP pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import praw\n",
    "import spacy\n",
    "import glob\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "stop_words =  set(stopwords.words('english'))\n",
    "\n",
    "def is_punct_space(token):\n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "direc = '../FinalTextFiles/'\n",
    "newStopWords = ['-PRON-']\n",
    "\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "plotly.tools.set_credentials_file(username='???', api_key='???')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Open and read the txt into dataframes\n",
    "os.chdir(direc)\n",
    "text = []\n",
    "ffiles = [f for f in os.listdir(direc) if os.path.isfile(f)]\n",
    "for f in ffiles:\n",
    "  with io.open (f, \"r\", encoding = 'cp437') as myfile:\n",
    "    text.append(myfile.read())\n",
    "\n",
    "data = pd.DataFrame({'raw':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This code is adapted from http://acsweb.ucsd.edu/~btomosch/ironfist.html\n",
    "#This function cleans up the dataframe, removes weird characters and spaces\n",
    "def cleaning_pipeline(data):\n",
    "    data = data.reset_index(drop=True)\n",
    "    #Turn line breaks into soaces\n",
    "    data['raw'] = [r.replace('\\n', ' ') for r in data['raw']]\n",
    "    data['raw'] = [r.replace('ΓÇÖ', \"'\") for r in data['raw']]\n",
    "    data['raw'] = data['raw'].astype(str)\n",
    "    #Split files by searching for Names in all uppercase + :\n",
    "    data2 = data.iloc[2:]\n",
    "    data2['split'] = [re.split(r\"\\s(?=[A-Z]+:)\",x) for x in data2.raw]\n",
    "    #Add episodes\n",
    "    data3 = pd.DataFrame(data2.split.values.tolist(), index= data2.index)\n",
    "    data3['episode'] = data3.index - 1\n",
    "    data4 = pd.melt(data3, id_vars=['episode'])\n",
    "    data4['value'].str.split(':',1, expand = True)\n",
    "    data5 = pd.concat([data4, data4['value'].str.split(':',1, expand = True)], axis=1)\n",
    "    data5 = data5.dropna().rename(index=str, columns={0: \"Speaker\", 1: \"Speech\"})\n",
    "    data5.drop(['variable', 'value'], axis=1, inplace = True)\n",
    "    data,data2,data3,data4 = None,None,None,None\n",
    "    return data5\n",
    "\n",
    "#This parses all of the text and removes custom stop words\n",
    "def nlp_pipeline(data):\n",
    "    \n",
    "    data['Speech'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "    data['Speech'].str.replace(r\"\\[.*\\]\",\"\")\n",
    "    #Parse and lemmatize the data\n",
    "    data['parsed'] = [ nlp(x) for x in data.Speech]\n",
    "    data['lemmatized'] = [[token.lemma_ for token in x\n",
    "                            if not is_punct_space(token)] \n",
    "                  for x in data.parsed]\n",
    "    \n",
    "    #Remove Stop words\n",
    "    data['cleaned'] = [[term for term in x\n",
    "                                if not term in newStopWords]\n",
    "                               for x in data.lemmatized]\n",
    "    #Collapse into one string rather than list\n",
    "    data.cleaned = [' '.join(x) for x in data.cleaned]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Speech</th>\n",
       "      <th>parsed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Hello everyone, and welcome to the second epi...</td>\n",
       "      <td>( , Hello, everyone, ,, and, welcome, to, the,...</td>\n",
       "      <td>[hello, everyone, and, welcome, to, the, secon...</td>\n",
       "      <td>hello everyone and welcome to the second episo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Hey, everyone. Sorry about that little issue ...</td>\n",
       "      <td>( , Hey, ,, everyone, ., Sorry, about, that, l...</td>\n",
       "      <td>[hey, everyone, sorry, about, that, little, is...</td>\n",
       "      <td>hey everyone sorry about that little issue the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Everyone, welcome to the new episode of Criti...</td>\n",
       "      <td>( , Everyone, ,, welcome, to, the, new, episod...</td>\n",
       "      <td>[everyone, welcome, to, the, new, episode, of,...</td>\n",
       "      <td>everyone welcome to the new episode of critica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Hello everyone, welcome to the fifth episode ...</td>\n",
       "      <td>( , Hello, everyone, ,, welcome, to, the, fift...</td>\n",
       "      <td>[hello, everyone, welcome, to, the, fifth, epi...</td>\n",
       "      <td>hello everyone welcome to the fifth episode of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Hello everyone! Welcome to Critical Role toni...</td>\n",
       "      <td>( , Hello, everyone, !, Welcome, to, Critical,...</td>\n",
       "      <td>[hello, everyone, welcome, to, critical, role,...</td>\n",
       "      <td>hello everyone welcome to critical role tonigh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode Speaker                                             Speech  \\\n",
       "0        1    MATT   Hello everyone, and welcome to the second epi...   \n",
       "1        2    MATT   Hey, everyone. Sorry about that little issue ...   \n",
       "2        3    MATT   Everyone, welcome to the new episode of Criti...   \n",
       "3        4    MATT   Hello everyone, welcome to the fifth episode ...   \n",
       "4        5    MATT   Hello everyone! Welcome to Critical Role toni...   \n",
       "\n",
       "                                              parsed  \\\n",
       "0  ( , Hello, everyone, ,, and, welcome, to, the,...   \n",
       "1  ( , Hey, ,, everyone, ., Sorry, about, that, l...   \n",
       "2  ( , Everyone, ,, welcome, to, the, new, episod...   \n",
       "3  ( , Hello, everyone, ,, welcome, to, the, fift...   \n",
       "4  ( , Hello, everyone, !, Welcome, to, Critical,...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  [hello, everyone, and, welcome, to, the, secon...   \n",
       "1  [hey, everyone, sorry, about, that, little, is...   \n",
       "2  [everyone, welcome, to, the, new, episode, of,...   \n",
       "3  [hello, everyone, welcome, to, the, fifth, epi...   \n",
       "4  [hello, everyone, welcome, to, critical, role,...   \n",
       "\n",
       "                                             cleaned  \n",
       "0  hello everyone and welcome to the second episo...  \n",
       "1  hey everyone sorry about that little issue the...  \n",
       "2  everyone welcome to the new episode of critica...  \n",
       "3  hello everyone welcome to the fifth episode of...  \n",
       "4  hello everyone welcome to critical role tonigh...  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = cleaning_pipeline(data)\n",
    "data = nlp_pipeline(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! The above step takes a long time, since we're building a dataframe with multiple copies of the parsed data in it. Only the episode, speaker, and cleaned columns are ultimately necessary, but I like showing the process from raw text to a single string of language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a dataframe of the counts for each word and speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The words we want to search for in the text\n",
    "words = ['elemental','uncanny dodge','bad news','rage']\n",
    "\n",
    "#The people we want to include in the analysis\n",
    "peeps = ['SAM','LAURA','TRAVIS','MATT','LIAM','TALIESIN','MARISHA', 'ASHLEY']\n",
    "\n",
    "\n",
    "#Use this to pickle the data - the dash code uses this\n",
    "#data[['episode','Speaker','cleaned']].to_pickle(\"../cleandata.pk1\")\n",
    "\n",
    "#Use this to filter out episodes\n",
    "data_subset = data[data['episode'] >= 1][data['episode'] <= 118]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only need to go to here and pickle the data if you use the Dash script to generate a python app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAM</td>\n",
       "      <td>go log on to geekandsundry.com slash find on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LAURA</td>\n",
       "      <td>cassandra what be do everybody except for some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAVIS</td>\n",
       "      <td>right listen up if have ale then have a friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MATT</td>\n",
       "      <td>hello everyone and welcome to the second episo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LIAM</td>\n",
       "      <td>between 1:00 and 4:00 be when be do stuff be a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  speaker                                               text\n",
       "0     SAM  go log on to geekandsundry.com slash find on t...\n",
       "1   LAURA  cassandra what be do everybody except for some...\n",
       "2  TRAVIS  right listen up if have ale then have a friend...\n",
       "3    MATT  hello everyone and welcome to the second episo...\n",
       "4    LIAM  between 1:00 and 4:00 be when be do stuff be a..."
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge all episodes into one string per speaker\n",
    "fullmerge = {}\n",
    "for peep in peeps:\n",
    "    fullmerge.update({peep:' '.join(data_subset[data_subset.Speaker == peep]['cleaned'])})\n",
    "finaldata = pd.DataFrame.from_dict(fullmerge, orient = 'index').reset_index().rename(columns = {'index':'speaker',0:'text'})\n",
    "finaldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe and populate it with the numbers for each of the above words for each person\n",
    "freq = pd.DataFrame()\n",
    "for peep in peeps:\n",
    "    for word in words:\n",
    "        freq = freq.append(pd.Series([peep, word, finaldata[finaldata['speaker'] == peep].text.str.count(word).iloc[0]]), ignore_index = True)\n",
    "        \n",
    "freq = freq.rename(index=str, columns={0: \"speaker\", 1: \"word\",2: \"amount\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>word</th>\n",
       "      <th>amount</th>\n",
       "      <th>total_x</th>\n",
       "      <th>rate</th>\n",
       "      <th>total_y</th>\n",
       "      <th>total_x</th>\n",
       "      <th>total_y</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAVIS</td>\n",
       "      <td>rage</td>\n",
       "      <td>209.0</td>\n",
       "      <td>188049.0</td>\n",
       "      <td>1.111412</td>\n",
       "      <td>188049.0</td>\n",
       "      <td>188049.0</td>\n",
       "      <td>188049.0</td>\n",
       "      <td>188049.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MARISHA</td>\n",
       "      <td>elemental</td>\n",
       "      <td>267.0</td>\n",
       "      <td>253320.0</td>\n",
       "      <td>1.054003</td>\n",
       "      <td>253320.0</td>\n",
       "      <td>253320.0</td>\n",
       "      <td>253320.0</td>\n",
       "      <td>253320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TALIESIN</td>\n",
       "      <td>bad news</td>\n",
       "      <td>114.0</td>\n",
       "      <td>242837.0</td>\n",
       "      <td>0.469451</td>\n",
       "      <td>242837.0</td>\n",
       "      <td>242837.0</td>\n",
       "      <td>242837.0</td>\n",
       "      <td>242837.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LIAM</td>\n",
       "      <td>uncanny dodge</td>\n",
       "      <td>75.0</td>\n",
       "      <td>228410.0</td>\n",
       "      <td>0.328357</td>\n",
       "      <td>228410.0</td>\n",
       "      <td>228410.0</td>\n",
       "      <td>228410.0</td>\n",
       "      <td>228410.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MATT</td>\n",
       "      <td>elemental</td>\n",
       "      <td>435.0</td>\n",
       "      <td>1648647.0</td>\n",
       "      <td>0.263853</td>\n",
       "      <td>1648647.0</td>\n",
       "      <td>1648647.0</td>\n",
       "      <td>1648647.0</td>\n",
       "      <td>1648647.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     speaker           word  amount    total_x      rate    total_y  \\\n",
       "3     TRAVIS           rage   209.0   188049.0  1.111412   188049.0   \n",
       "19   MARISHA      elemental   267.0   253320.0  1.054003   253320.0   \n",
       "7   TALIESIN       bad news   114.0   242837.0  0.469451   242837.0   \n",
       "27      LIAM  uncanny dodge    75.0   228410.0  0.328357   228410.0   \n",
       "23      MATT      elemental   435.0  1648647.0  0.263853  1648647.0   \n",
       "\n",
       "      total_x    total_y      total  \n",
       "3    188049.0   188049.0   188049.0  \n",
       "19   253320.0   253320.0   253320.0  \n",
       "7    242837.0   242837.0   242837.0  \n",
       "27   228410.0   228410.0   228410.0  \n",
       "23  1648647.0  1648647.0  1648647.0  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the total number of words spoken by each speaker - estimated as the number of spaces.\n",
    "totalwords = pd.DataFrame()\n",
    "for peep in peeps:\n",
    "    totalwords = totalwords.append(pd.Series([peep, ' ', finaldata[finaldata['speaker'] == peep].text.str.count(' ').iloc[0]]), ignore_index = True)\n",
    "\n",
    "totalwords = totalwords.rename(index=str, columns={0: \"speaker\", 1: \"word\",2: \"total\"})\n",
    "\n",
    "freq = pd.merge(freq, totalwords[['speaker','total']], on='speaker')\n",
    "\n",
    "#Calculate the rate of times the word is said per 1000 words\n",
    "freq['rate'] = (freq['amount']/freq['total'])*1000\n",
    "totalwords = None\n",
    "\n",
    "#sort by rate and speaker\n",
    "freq = freq.sort_values(by=['rate','speaker'], ascending = [False,True])\n",
    "freq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have a dataframe that shows the frequency and rates of each word for each speaker. Lastly we will make the graph with plotly, since the dashboard package (Dash) uses plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~btomoschuk/4.embed\" height=\"600px\" width=\"800px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = ff.create_facet_grid(\n",
    "    freq,\n",
    "    x='rate',\n",
    "    y='word',\n",
    "    facet_col='speaker',\n",
    "    color_name='speaker',\n",
    "    trace_type='bar',\n",
    "    orientation = 'h',\n",
    "    ggplot2 = True\n",
    ")\n",
    "\n",
    "#Change the axes to match the minimum and 15% over the maximum rates for nice graphing\n",
    "for i in range(len(peeps)+1):\n",
    "    if i == 0:\n",
    "        fig.layout.xaxis.update({'range': [freq['rate'].min(), (freq['rate'].max()+(.15 * freq['rate'].max()))]})\n",
    "    else:\n",
    "        exec('fig.layout.xaxis' + str(i)+\".update({'range': [freq['rate'].min(), (freq['rate'].max()+(.15 * freq['rate'].max()))]})\")\n",
    "\n",
    "fig.layout.update(plot_bgcolor='rgba(230,230,230,90)')\n",
    "py.iplot(fig, filename='critrole')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See github.com/tomoschuk for the accompanying Dash script that generates crtranscript.herokuapp.com!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
